{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIdCc7PEUi/PY/DpdNqxhX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IzWclXwKBqzP"},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","from transformers import AutoTokenizer, AutoModel\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.preprocessing import StandardScaler\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\n","from sklearn.model_selection import GridSearchCV\n","from transformers import AutoModelForSeq2SeqLM\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","import time\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","class CodeDataset(Dataset):\n","    def __init__(self, features, labels):\n","        self.features = torch.FloatTensor(features)\n","        self.labels = torch.LongTensor(labels)\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        return self.features[idx], self.labels[idx]\n","\n","class EnhancedCodeAnalyzer:\n","    def __init__(self, codebert_model_name=\"microsoft/codebert-base\", codet5_model_name=\"Salesforce/codet5-base\"):\n","        drive.mount('/content/drive', force_remount=True)\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","        # CodeBERT\n","        self.codebert_tokenizer = AutoTokenizer.from_pretrained(codebert_model_name)\n","        self.codebert_model = AutoModel.from_pretrained(\n","            codebert_model_name,\n","            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n","            low_cpu_mem_usage=True,\n","            trust_remote_code=True\n","        ).to(self.device)\n","        # CodeT5\n","        self.codet5_tokenizer = AutoTokenizer.from_pretrained(codet5_model_name)\n","        self.codet5_model = AutoModelForSeq2SeqLM.from_pretrained(\n","            codet5_model_name,\n","            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n","            low_cpu_mem_usage=True,\n","            trust_remote_code=True\n","        ).to(self.device)\n","        self.harmless_path = '/content/drive/MyDrive/yukseklisans/metinmadenciligi/Dataset/GPT3.5/Harmless'\n","        self.malicious_path = '/content/drive/MyDrive/yukseklisans/metinmadenciligi/Dataset/GPT3.5/Malicious'\n","\n","    def load_code_files(self, directory):\n","        code_files = []\n","        for filename in os.listdir(directory):\n","            if filename.endswith('.txt'):\n","                with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n","                    code_files.append(file.read())\n","        return code_files\n","\n","    def prepare_dataset(self):\n","        harmless_codes = self.load_code_files(self.harmless_path)\n","        malicious_codes = self.load_code_files(self.malicious_path)\n","\n","        dataset = pd.DataFrame({\n","            'code': harmless_codes + malicious_codes,\n","            'label': ['harmless'] * len(harmless_codes) + ['malicious'] * len(malicious_codes)\n","        })\n","        return dataset\n","\n","    def get_codebert_embedding(self, code_snippet):\n","        inputs = self.codebert_tokenizer(code_snippet, return_tensors='pt', max_length=512, truncation=True,\n","                                         padding=True).to(self.device)\n","        with torch.no_grad():\n","            outputs = self.codebert_model(**inputs)\n","        embeddings = outputs.last_hidden_state.mean(dim=1)\n","        return embeddings.cpu().numpy()\n","\n","    def get_codet5_embedding(self, code_snippet):\n","        inputs = self.codet5_tokenizer(code_snippet, return_tensors='pt', max_length=512, truncation=True,\n","                                         padding=True).to(self.device)\n","        with torch.no_grad():\n","            outputs = self.codet5_model.encoder(**inputs)\n","        embeddings = outputs.last_hidden_state.mean(dim=1)\n","        return embeddings.cpu().numpy()\n","\n","    def prepare_all_feature_sets(self, dataset):\n","        codebert_embeddings = self.prepare_codebert_embeddings(dataset)\n","        codet5_embeddings = self.prepare_codet5_embeddings(dataset)\n","\n","        feature_sets = {\n","            \"codebert\": codebert_embeddings,\n","            \"codet5\": codet5_embeddings\n","        }\n","        return feature_sets\n","\n","    def prepare_codebert_embeddings(self, dataset):\n","        embeddings = [self.get_codebert_embedding(code) for code in dataset['code']]\n","        return np.vstack(embeddings)\n","\n","    def prepare_codet5_embeddings(self, dataset):\n","        embeddings = [self.get_codet5_embedding(code) for code in dataset['code']]\n","        return np.vstack(embeddings)\n","\n","    def prepare_labels(self, dataset):\n","        return dataset['label'].apply(lambda x: 1 if x == 'malicious' else 0).values\n","\n","    def visualize_features(self, feature_sets, labels, output_dir=\"/content/drive/MyDrive/yukseklisans/metinmadenciligi/Dataset/GPT3.5/feature_outputs\"):\n","        os.makedirs(output_dir, exist_ok=True)\n","        for key, features in feature_sets.items():\n","            df = pd.DataFrame(features[:, :2], columns=['Feature_1', 'Feature_2'])  # Ä°lk 2 feature'Ä± al\n","            df['label'] = labels\n","\n","            plt.figure(figsize=(8, 6))\n","            sns.scatterplot(x=\"Feature_1\", y=\"Feature_2\", hue=\"label\", data=df, palette=\"coolwarm\", alpha=0.7) # data parametresi eklendi\n","            plt.title(f\"Feature Distribution - {key}\")\n","            plt.xlabel(\"Feature 1\")\n","            plt.ylabel(\"Feature 2\")\n","            plt.legend(title=\"Label\")\n","\n","            image_path = os.path.join(output_dir, f\"{key}_feature_visualization.png\")\n","            plt.savefig(image_path)\n","            print(f\"ðŸ“Š Feature visualization saved: {image_path}\")\n","            plt.show()\n","\n","\n","# Veri hazÄ±rlama\n","analyzer = EnhancedCodeAnalyzer()\n","dataset = analyzer.prepare_dataset()\n","feature_sets = analyzer.prepare_all_feature_sets(dataset)\n","y = analyzer.prepare_labels(dataset)\n","\n","scaled_feature_sets = {}\n","for key, features in feature_sets.items():\n","    scaler = StandardScaler()\n","    scaled_feature_sets[key] = scaler.fit_transform(features)\n","\n","# Train-test split\n","train_test_data = {}\n","for key, X in scaled_feature_sets.items():\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","    train_test_data[key] = {\n","        'X_train': X_train,\n","        'X_test': X_test,\n","        'y_train': y_train,\n","        'y_test': y_test\n","    }\n","\n","# SÄ±nÄ±flandÄ±rÄ±cÄ±lar ve GridSearch hiperparametreleri\n","classifiers = {\n","    'GaussianNB': GaussianNB(),\n","    'Logistic Regression': LogisticRegression(random_state=42),\n","    'KNN': KNeighborsClassifier(),\n","    'Decision Tree': DecisionTreeClassifier(random_state=42),\n","    'SVM': SVC(random_state=42),\n","    'Random Forest': RandomForestClassifier(random_state=42),\n","    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n","    'AdaBoost': AdaBoostClassifier(random_state=42)\n","}\n","\n","# Ã–zelleÅŸtirilmiÅŸ deÄŸerlendirme metrikleri\n","scoring = {\n","    'precision': make_scorer(precision_score, pos_label=1),\n","    'recall': make_scorer(recall_score, pos_label=1),\n","    'accuracy': make_scorer(accuracy_score),\n","    'f1': make_scorer(f1_score, pos_label=1)\n","}\n","\n","# GridSearch iÃ§in hiperparametre aralÄ±klarÄ±\n","param_grids = {\n","    'GaussianNB': {},\n","    'Logistic Regression': {\n","        'C': [0.1, 1, 10],\n","        'penalty': ['l2'],\n","        'solver': ['liblinear', 'lbfgs']\n","    },\n","    'KNN': {\n","        'n_neighbors': [3, 5, 7, 9],\n","        'weights': ['uniform', 'distance'],\n","        'metric': ['euclidean', 'manhattan']\n","    },\n","     'Decision Tree':{\n","        'max_depth': [None, 5, 10],\n","        'min_samples_split': [2, 5],\n","        'min_samples_leaf': [1, 2]\n","    },\n","        'SVM': {\n","        'C': [0.1, 1, 10],\n","        'gamma': ['scale', 'auto'],\n","        'kernel': ['rbf', 'linear']\n","    },\n","    'Random Forest': {\n","        'n_estimators': [50, 100],\n","        'max_depth': [None, 5, 10],\n","        'min_samples_split': [2, 5],\n","        'min_samples_leaf': [1, 2]\n","    },\n","    'Gradient Boosting': {\n","        'n_estimators': [50, 100],\n","        'learning_rate': [0.01, 0.1],\n","        'max_depth': [3, 5]\n","    },\n","        'AdaBoost': {\n","        'n_estimators': [50, 100, 200],\n","        'learning_rate': [0.01, 0.1, 0.2]\n","    }\n","}\n","# GridSearch sonuÃ§larÄ±nÄ± depolamak iÃ§in\n","grid_search_results = {}\n","\n","# Her veri kÃ¼mesi ve sÄ±nÄ±flandÄ±rÄ±cÄ± iÃ§in GridSearch uygulama\n","for feature_set_name, data in train_test_data.items():\n","    X_train = data['X_train']\n","    X_test = data['X_test']\n","    y_train = data['y_train']\n","    y_test = data['y_test']\n","    print(f\"\\n==================================================\")\n","    print(f\"Veri KÃ¼mesi: {feature_set_name}\")\n","    print(f\"==================================================\")\n","    for name, clf in classifiers.items():\n","        print(f\"\\n{name} iÃ§in GridSearch Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...\")\n","        param_grid = param_grids[name]\n","\n","        # Stratified K-Fold oluÅŸtur\n","        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","        grid_search = GridSearchCV(\n","            estimator=clf,\n","            param_grid=param_grid,\n","            scoring=scoring,\n","            refit='f1',  # En iyi modeli F1 skora gÃ¶re seÃ§\n","            cv=cv,  # Stratified K-Fold kullan\n","            verbose=2,\n","            n_jobs=-1\n","        )\n","        start_time = time.time()\n","        grid_search.fit(X_train, y_train)\n","        end_time = time.time()\n","        training_time = end_time - start_time\n","\n","        # En iyi skor ve parametreleri kaydet\n","        best_params = grid_search.best_params_\n","        best_f1_score = grid_search.cv_results_['mean_test_f1'][grid_search.best_index_]\n","        best_precision = grid_search.cv_results_['mean_test_precision'][grid_search.best_index_]\n","        best_recall = grid_search.cv_results_['mean_test_recall'][grid_search.best_index_]\n","        best_accuracy = grid_search.cv_results_['mean_test_accuracy'][grid_search.best_index_]\n","\n","        grid_search_results[(feature_set_name, name)] = {\n","            'best_params': best_params,\n","            'metrics': {\n","                'f1': best_f1_score,\n","                'precision': best_precision,\n","                'recall': best_recall,\n","                'accuracy': best_accuracy\n","            },\n","            'training_time': training_time\n","        }\n","\n","        print(f\"\\n{name} iÃ§in En Ä°yi Performans:\")\n","        print(f\"F1 Skor: {best_f1_score}\")\n","        print(f\"Precision: {best_precision}\")\n","        print(f\"Recall: {best_recall}\")\n","        print(f\"Accuracy: {best_accuracy}\")\n","        print(f\"EÄŸitim SÃ¼resi: {training_time:.2f} saniye\")\n","        print(f\"En Ä°yi Hiperparametreler: {best_params}\")\n","\n","\n","# GridSearch sonuÃ§larÄ±nÄ± yazdÄ±rma\n","print(\"\\nTÃ¼m Modeller Ä°Ã§in GridSearch SonuÃ§larÄ±:\")\n","for (feature_set_name, model_name), result in grid_search_results.items():\n","    print(f\"\\nVeri KÃ¼mesi: {feature_set_name}, Model: {model_name}\")\n","    if 'best_params' in result:\n","        print(f\"  En Ä°yi Hiperparametreler = {result['best_params']}\")\n","    print(f\"  Performans:\")\n","    for metric, score in result['metrics'].items():\n","        print(f\"    {metric.capitalize()}: {score:.4f}\")\n","    print(f\"   EÄŸitim SÃ¼resi: {result['training_time']:.2f} saniye\")\n","\n","# GÃ¶rselleÅŸtirme\n","analyzer.visualize_features(scaled_feature_sets, y)\n","\n","# **CSV DosyalarÄ±na Kaydetme**\n","output_dir = \"/content/drive/MyDrive/yukseklisans/metinmadenciligi/Dataset/GPT3.5/feature_outputs\"  # Google Drive'daki hedef dizin\n","os.makedirs(output_dir, exist_ok=True)\n","\n","for key, features in scaled_feature_sets.items():\n","    df = pd.DataFrame(features)\n","    df['label'] = y  # Etiketleri ekle\n","    csv_path = os.path.join(output_dir, f\"{key}_features.csv\")\n","    df.to_csv(csv_path, index=False)\n","    print(f\"âœ” Feature set '{key}' kaydedildi: {csv_path}\")"]}]}